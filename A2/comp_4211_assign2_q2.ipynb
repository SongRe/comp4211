{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzlrOzuqnEIP"
   },
   "source": [
    "# Introduction of Q2\n",
    "In this question, you will utilize Torch to implement a Recurrent Neural Network, specifically the LSTM model, for sentiment analysis. Sentiment analysis involves classifying the emotional tone of a sentence as either positive or negative. You will work with a preprocessed dataset consisting of 25,000 sentences about movie review, which are stored in three files: 'label.pkl', 'encoded_input.pkl', and 'vocab_to_int.pkl'. The 'label.pkl' file contains the sentence labels, while the 'encoded_input.pkl' file contains the encoded IDs of the sentences, where each word has its own ID. The translation dictionary is saved in the 'vocab_to_int.pkl' file. Your objective will be to preprocess the data, implement the model, and train and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyt2In2xqd5Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0jpLYusqmI8"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1oJWIa3esCmlHeICCqtLVoIYe65IdXqwl'  -c -O label.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vuW6eDpScDoyiJ66K328mYg0Ky6hZrij'  -c -O encoded_input.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1lON426HKNYnONwUn6TtHnd3zIq2JbBBJ'  -c -O vocab_to_int.pkl\n",
    "!mv *.pkl data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxs3c6k7eOMm"
   },
   "outputs": [],
   "source": [
    "## load dataset\n",
    "with open('./data/encoded_input.pkl', 'rb') as f:\n",
    "  encoded_input = pickle.load(f)\n",
    "\n",
    "with open('./data/label.pkl', 'rb') as f:\n",
    "  labels = pickle.load(f)\n",
    "\n",
    "with open('./data/vocab_to_int.pkl', 'rb') as f:\n",
    "  vocab_to_int = pickle.load(f)\n",
    "\n",
    "\n",
    "print(labels[:10])\n",
    "print(encoded_input[:10])\n",
    "print(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gV8wUHAff7M0"
   },
   "outputs": [],
   "source": [
    "## pad the encoded input to the same shape\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's\n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    ## getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    ## for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drAuW0_KD5zF"
   },
   "outputs": [],
   "source": [
    "# The maximum length is too large, so we set the maximum length to 200\n",
    "max_length = max(len(x) for x in encoded_input)\n",
    "print('original maximum length: ', max_length)\n",
    "\n",
    "max_length_padding = 200\n",
    "padded_input = pad_features(encoded_input, max_length_padding)\n",
    "print('new maximum length: ', max_length_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1crivKeWE-eX"
   },
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "eval_frac = 0.1\n",
    "\n",
    "total_size = len(labels)\n",
    "\n",
    "# split the dataset for training, validation and evaluation.\n",
    "\"\"\"\n",
    "Begining of of Implement\n",
    "\n",
    "1. split padded_input into variables \"train_x\", \"val_x\", \"test_x\";\n",
    "2. split labels into variables \"train_y\", \"val_y\", \"test_y\";\n",
    "\"\"\"\n",
    "\n",
    "# your code here\n",
    "\n",
    "\"\"\"\n",
    "End of Implement\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeatures Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpZdIItNpmwU"
   },
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# SHUFFLE data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xIULz1Zpmwf"
   },
   "outputs": [],
   "source": [
    "# checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJHNs4FZpmwj"
   },
   "outputs": [],
   "source": [
    "## Define the recurrent neural network\n",
    "\n",
    "class RNN_model(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob):\n",
    "        super(RNN_model, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        \"\"\"\n",
    "        Begining of of Implement\n",
    "\n",
    "        1. define self.embedding (vocab_size * embedding_dim) for the vocabulary using nn.Embedding\n",
    "        2. define self.lstm with input of embedding_dim, hidden_dim, n_layers, and dropout probability of drop_prob.\n",
    "        Note that batch_first in lstm need to be set to 'True' to process the shape of (batch_size, seq_len, features)\n",
    "        3. add a dropout layer self.dropout with probability of 0.3 before the linear layer.\n",
    "        4. add a linear layer self.fc (hidden_dim * outputsize) and a sigmoid output layer self.sig.\n",
    "        \"\"\"\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        \"\"\"\n",
    "        End of Implement\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Perform a forward pass of our model on some input and hidden state.\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Begining of of Implement\n",
    "\n",
    "        1. get the embeddings of x using self.embedding -> variable \"embeds\".\n",
    "        2. get the output of lstm using self.lstm with inputs \"embeds\" and \"hidden\" -> \"lstm_out\" and \"hidden\".\n",
    "        3. get the output of the dropout layer and the linear layer -> variable \"out\".\n",
    "        4. get the output of the output layer -> variable \"sig_out\".\n",
    "        5. return the last sigmoid output and the hidden state -> variables \"sig_out\" and \"hidden\"\n",
    "        \"\"\"\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        \"\"\"\n",
    "        End of Implement\n",
    "        \"\"\"\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if(train_on_gpu):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZARxJ74pmwn"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400 # size of the embeddings\n",
    "hidden_dim = 256    # Number of units in the hidden layers of our LSTM cells\n",
    "n_layers = 2        # Number of LSTM layers\n",
    "dropout_prob = 0.5\n",
    "\n",
    "\"\"\"\n",
    "Begining of of Implement\n",
    "\n",
    "Define the RNN model using the above RNN_model class and hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# your code here\n",
    "\n",
    "\"\"\"\n",
    "End of Implement\n",
    "\"\"\"\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLmCzDanpmwq"
   },
   "outputs": [],
   "source": [
    "# learning rate, loss function, and optimization functions\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJqrvKg0pmwu"
   },
   "outputs": [],
   "source": [
    "# Begin to train the networks\n",
    "# training params\n",
    "epochs = 4\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5  # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        \"\"\"\n",
    "        Begining of of Implement\n",
    "        do the optimization step:\n",
    "        1. get the output from the model -> variables \"output\" and \"h\".\n",
    "        2. calculate the loss using criterion, output, and the labels -> variables \"loss\".\n",
    "        3. clean the old/previous gradient;\n",
    "        4. compute the current gradient (backward propagation)\n",
    "        5. update the parameter\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "\n",
    "        \"\"\"\n",
    "        End of Implement\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd_GWDE0pmwz"
   },
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Begining of of Implement\n",
    "    do the optimization step:\n",
    "    1. get the output from the model -> variables \"output\" and \"h\".\n",
    "    2. calculate the loss using criterion, output, and the labels -> variables \"test_loss\".\n",
    "    3. append the \"test_loss\" into test_losses.\n",
    "    4. convert output probabilities to predicted class (0 or 1)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    \"\"\"\n",
    "    End of Implement\n",
    "    \"\"\"\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcrLL0YZYvPo"
   },
   "outputs": [],
   "source": [
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuatuon\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUZJ4mCpmw6"
   },
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    ''' Prints out whether a give review is predicted to be\n",
    "        positive or negative in sentiment, using a trained model.\n",
    "\n",
    "        params:\n",
    "        net - A trained net\n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "    '''\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "\n",
    "    # pad tokenize sequence\n",
    "    seq_length = sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "\n",
    "    # convert to tensor to pass to model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "\n",
    "    batch_size = feature_tensor.size(0)\n",
    "\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "      feature_tensor = feature_tensor.cuda()\n",
    "\n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "\n",
    "    # print custom response based on whether test_review is pos/neg\n",
    "    if(pred.item()==1):\n",
    "      print('Positive review detected!')\n",
    "    else:\n",
    "      print('Negative review detected!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6eLet-tpmw9"
   },
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIB-pSP1pmxB"
   },
   "outputs": [],
   "source": [
    "# call function to test your model!\n",
    "seq_length=200\n",
    "predict(net, test_review_pos, seq_length)\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
